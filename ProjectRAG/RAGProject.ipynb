{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f1f39d-96f6-44a7-99d9-902a2282c968",
   "metadata": {},
   "source": [
    "# Project: Question Answering on Private Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de830de-8f2b-4395-acdf-dffa03f3aacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7088f59a-1f01-430a-9158-bdd408bd9038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be58554-a3d5-4833-b621-db93b3475d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7930165c-2066-4e20-9b82-f3062fe488ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1478c92-ac72-4eed-96cc-c1cfe243f3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae9636c8-8a5d-4ca7-813c-5f1973d2c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain_community.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "# Wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain_community.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b01c39d-d2c9-4d77-87ee-c86701ece29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192775d-2f5d-408e-9e7e-c0de8dc8a020",
   "metadata": {},
   "source": [
    "## Embeding Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9b4abf8-8845-4477-9163-135532808e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00255:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64087fa-6b89-4d2c-8c26-2a3f5ed05593",
   "metadata": {},
   "source": [
    "## Embedding and Uploading to a Vector Database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c104babd-9793-4b78-b8a7-4975529843a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    pc = pinecone.Pinecone()\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('OK')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(environment='gcp-starter')\n",
    "        )\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('OK')\n",
    "        return vector_store\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b14eb5d-adc5-4341-9029-d769e9709fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ...')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('OK')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9078ef-086e-4e78-ae27-43b589984823",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0c02d7c-70c5-484d-a2fd-b26e047bc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.run(q)\n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd745bb-f1cf-4d9f-a52e-b6d880c5f85c",
   "metadata": {},
   "source": [
    "## Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea5fedc6-fc4d-45ec-8979-12bb7c5253b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/us_constitution.pdf\n",
      "You have 41 pages in your data\n",
      "There are 1137 characters in the page\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/us_constitution.pdf')\n",
    "# print(data[1],page_content)\n",
    "# print(data[10].metadata)\n",
    "\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[20].page_content)} characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d3db9d0-fe1f-493a-8b5a-ed99e38d6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_document('files/the_great_gatsby.docx')\n",
    "# print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b92982ad-7456-42a4-a619-ec185a79b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_from_wikipedia('GPT-4', 'de')\n",
    "# print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2787d0a-a45f-49f7-8961-f300328d6904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "Representatives\n",
      "shall\n",
      "chuse\n",
      "their\n",
      "Speaker\n",
      "and\n",
      "other\n",
      "Of ficers;and\n",
      "shall\n",
      "have\n",
      "the\n",
      "sole\n",
      "Power\n",
      "of\n",
      "Impeachment.\n",
      "Section\n",
      "3:\n",
      "The\n",
      "Senate\n",
      "The\n",
      "Senate\n",
      "of\n",
      "the\n",
      "United\n",
      "States\n",
      "shall\n",
      "be\n",
      "composed\n",
      "of\n",
      "two\n",
      "Senators\n",
      "from\n",
      "each\n",
      "State,\n",
      "chosen\n",
      "by\n",
      "the\n",
      "Legislature\n",
      "thereof,\n",
      "for\n",
      "six\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c699f0-a40f-44f3-b2d8-65c882155106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 16711\n",
      "Embedding Cost in USD: 0.042613\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17bfd6c8-2ff4-4849-a20a-9b7e44384006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e07ecca2-b991-4ce6-88fb-68572a053c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...OK\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13f6c8eb-40a3-45a6-813e-aa2d4784c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasyidannas/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pieces of text provided are actually excerpts from the United States Constitution. The Constitution outlines the framework for the government of the United States, establishing the three branches of government (legislative, executive, and judicial) and defining the powers and limitations of each branch. It also guarantees certain rights and freedoms to the people and sets the foundation for the country's laws and governance.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a259353d-be2f-4d80-960f-2ef07daba442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting ... bye bye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66717647-302b-49a5-929c-ae07a0c9f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c85062c-eb0e-4e21-93f8-f40a185fdf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index chatgpt and embeddings ...OK\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('ChatGPT', 'id')\n",
    "chunks = chunk_data(data)\n",
    "index_name = 'chatgpt'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40616adf-456e-4258-a159-efa317c5d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maaf, saya tidak memiliki informasi terkait \"chatgpt\" dalam konteks yang diberikan. Apakah ada pertanyaan lain yang bisa saya bantu jawab?\n"
     ]
    }
   ],
   "source": [
    "q = \"Apa itu chatgpt\"\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd3a37-a5c5-4daa-8433-285ae6f57c99",
   "metadata": {},
   "source": [
    "## Using Chroma as a Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfad5db-5050-4a6e-9c82-36724729f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31aa72e9-9ad1-4150-ab80-fdc432ba2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  \n",
    "\n",
    "    # Create a Chroma vector store using the provided text chunks and embedding model, \n",
    "    # configuring it to save data to the specified directory \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory) \n",
    "\n",
    "    return vector_store  # Return the created vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65e70f1f-21ac-4a74-922e-549eca69884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate the same embedding model used during creation\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536) \n",
    "\n",
    "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings) \n",
    "\n",
    "    return vector_store  # Return the loaded vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86586f77-6766-40ef-9618-20a020148e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f7632a3-5d9d-480d-ae51-8957e1ea6be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI Search is a feature within Google's Vertex AI platform. It offers customizable answers, search tuning, vector search, grounding, and compliance updates for enterprises. It also includes new generative AI capabilities and enterprise-ready features to enhance AI and machine learning tasks related to search functions.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is Vertex AI Search'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c07ca03-9b02-4bc2-8c1d-6d8a3af6d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The StackOverflow dataset used in the context provided had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "# Load a Chroma vector store from the specified directory (default ./chroma_db) \n",
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808f457a-fa0a-4ccc-aee3-91a03dfa3e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but there is no specific number provided in the context that can be multiplied by 2.\n"
     ]
    }
   ],
   "source": [
    "q = 'Multiply that number by 2'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dd7b0-cb96-470b-8359-ca88e4ee543a",
   "metadata": {},
   "source": [
    "### Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1da3853d-cc7c-44cd-a702-4e44def98607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains \n",
    "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
    "\n",
    "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)  \n",
    "\n",
    "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})  \n",
    "\n",
    "\n",
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Link the ChatGPT LLM\n",
    "    retriever=retriever,  # Link the vector store based retriever\n",
    "    memory=memory,  # Link the conversation memory\n",
    "    chain_type='stuff',  # Specify the chain type\n",
    "    verbose=False  # Set to True to enable verbose logging for debugging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "695db368-dcf9-4c86-b7d2-76939f0e006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e9c5b9b-242d-4bb3-8933-a9aed1c909d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "878dd0af-274a-4f05-aa80-c874b21766ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='The StackOverflow dataset had 8 million pairs of questions and answers.')], 'answer': 'The StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac673992-d4c7-4992-a56c-7fd7378df1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The StackOverflow dataset had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9860156-8eb4-498f-ba3d-59d846ebbd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying 8 million pairs of questions and answers by 10 would be 80 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "q = 'Multiply that number by 10'\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c57817a-f8b4-421f-8389-dd26a25de85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of dividing 80 million pairs of questions and answers by 80 is 1 million.\n"
     ]
    }
   ],
   "source": [
    "q = 'Devide that result by 80'\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4345bbcd-f9bb-4fe5-9182-bed63332b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='How many pairs of questions and answers had the StackOverflow dataset?'\n",
      "content='The StackOverflow dataset had 8 million pairs of questions and answers.'\n",
      "content='Multiply that number by 10'\n",
      "content='Multiplying 8 million pairs of questions and answers by 10 would result in 80 million pairs of questions and answers.'\n",
      "content='Multiply that number by 10'\n",
      "content='The result of multiplying 8 million pairs of questions and answers by 10 would be 80 million pairs of questions and answers.'\n",
      "content='Devide that result by 80'\n",
      "content='The result of dividing 80 million pairs of questions and answers by 80 is 1 million.'\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586ff39-3dc6-424b-91d9-55303c2640da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
